# NGN Network Slicing Exam

## Overview

This project implements a network slicing architecture using Mininet, Ryu SDN controller, and WebSocket-based communication for dynamic flow allocation and monitoring. The system supports both **basic** and **slicing** modes for traffic management, enabling bandwidth reservation and QoS enforcement.

---

## Instructions and File Descriptions

### SSH into the VM

To connect to the VM (password: `vagrant`):

```bash
ssh -X -p 2222 vagrant@localhost
```

---

### Topology

The file **topology.py** defines the network topology, which is dynamically loaded from a YAML file (**topology.yaml**). The topology includes hosts, switches, and links with configurable bandwidth.

To run the topology in Mininet:

```bash
sudo python3 topology.py
```

This will:

- Load the topology from `topology.yaml`.
- Save host MAC addresses and switch link information to `/tmp/host_info.json` and `/tmp/switch_links_info.json`.
- Start a WebSocket server for Mininet commands on `ws://127.0.0.1:9876`.

---

### Controller

The file **flow_allocator_controller.py** is the Ryu-based SDN controller. It manages:

- Flow allocation and deletion.
- Bandwidth reservation using QoS queues.
- Dynamic topology updates.

To start the controller, use the provided shell script:

```bash
sudo ./start_controller.sh
```

This will also start a WebSocket server for flow management on `ws://127.0.0.1:8765`.

---

#### Automatic Allocation

Run the **tester.py** script to allocate flows interactively:

```bash
sudo python3 tester.py
```

This script:

- Reads host MAC addresses from `/tmp/host_info.json`.
- Allows you to select source and destination hosts.
- Sends WebSocket requests to allocate flows.

---

### WebSocket Communication

The system uses WebSocket servers for communication:

- **Mininet WebSocket Server** (`ws://127.0.0.1:9876`): Executes commands on Mininet hosts.
- **Controller WebSocket Server** (`ws://127.0.0.1:8765`): Handles flow allocation, deletion, and monitoring.

---

### Traffic Testing

The system supports two modes for traffic testing, configured via the `.env` file:

```properties
TEST_MODE=slicing
# TEST_MODE=basic
```

#### Slicing Mode

In slicing mode, traffic is managed with bandwidth reservations. To test slicing:

1. Run the topology:

   ```bash
   sudo python3 topology.py
   ```

2. Start the controller

   ```bash
   ./start_controller.sh
   ```

3. Allocate flows using the **tester.py** script:

   ```bash
   sudo python3 tester.py
   ```

4. Run the pre-built `iperf` command to generate traffic in the **tester.py**.

5. View throughput graphs generated by **graph_mesh_slice.py**.

#### Basic Mode

In basic mode, traffic is managed without slicing. Follow the same steps as slicing mode but set `TEST_MODE=basic` in the `.env` file. Graphs will be generated by **graph_mesh_basic.py**.

---

### Path Finder

The file **path_finder.py** implements the Widest Path algorithm for routing traffic between hosts. Instead of seeking the shortest path, it finds the path where the minimum bandwidth link (the bottleneck) has the maximum possible capacity - essentially selecting the path where the weakest link is as strong as possible. The path finder integrates with the controller to enforce appropriate QoS policies based on the selected path.

---

### Monitoring and Visualization

- **Throughput Graphs**: Generated by `graph_mesh_slice.py` or `graph_mesh_basic.py` and saved in the `netbench` directory.
- **Flow Reservations**: View active reservations using the `show` command in the CLI.

---

### CLI Commands

The CLI provides the following commands:

| Command    | Description                                 |
| ---------- | ------------------------------------------- |
| `allocate` | Allocate a new flow between two hosts.      |
| `delete`   | Delete an existing flow.                    |
| `dump`     | Dump flows from a specific switch.          |
| `show`     | Show the flow reservation table.            |
| `ping`     | Test connectivity between two hosts.        |
| `iperf`    | Run `iperf` tests based on the `TEST_MODE`. |
| `help`     | Show the help menu.                         |
| `clear`    | Clear the screen.                           |
| `exit`     | Exit the CLI.                               |

---

### Notes

- Ensure Mininet and the Ryu controller are running before testing.
- Open at least 3 terminal sessions for running the topology, controller, and CLI.
- Clean up QoS configurations using the `clean_ovs_qos()` function in **topology.py** if needed.

All set! :smile:
